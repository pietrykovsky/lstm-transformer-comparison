{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "filepath = \"cleaned_text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_unicode(text):\n",
    "    if isinstance(text, bytes):\n",
    "        return text.decode('utf-8', 'ignore')\n",
    "    elif isinstance(text, str):\n",
    "        return text\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported string type\")\n",
    "\n",
    "# Function to filter text\n",
    "def filter_text(text, allowed_chars):\n",
    "    return ''.join([char for char in text if char in allowed_chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n",
      "55474549\n"
     ]
    }
   ],
   "source": [
    "with open(filepath, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    content = file.read()\n",
    "    content = ensure_unicode(content)\n",
    "    all_chars = string.printable\n",
    "    filtered_text = filter_text(content, all_chars)\n",
    "\n",
    "# Use filtered text as 'all_text'\n",
    "all_text = filtered_text\n",
    "number_of_char = len(all_text)\n",
    "\n",
    "all_chars = string.printable\n",
    "number_of_chars = len(all_chars)\n",
    "print(all_chars)\n",
    "print(len(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embed(x)\n",
    "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "        return out, (hidden, cell)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.chunk_len = 256 # puścić zwiększone do 350 dla lr=0.003 lr=0.0035 i lr=0.004\n",
    "        self.num_epochs = 5000\n",
    "        self.batch_size = 1\n",
    "        self.print_every = 25\n",
    "        self.hidden_size = 256\n",
    "        self.num_layers = 3  # Nie ruszaj\n",
    "        self.lr = 0.0006 # puścić to oraz dodatkowo bez ruszania chunk_len lr=0.004\n",
    "        # czyli 5 puszczeń sieci :\n",
    "        # 1: chunk_len=256 lr=0.0035\n",
    "        # 2: chunk_len=256 lr=0.004\n",
    "        # 3: chunk_len=350 lr=0.003\n",
    "        # 4: chunk_len=350 lr=0.0035\n",
    "        # 5: chunk_len=350 lr=0.004\n",
    "        self.rnn = RNN(number_of_chars, self.hidden_size, self.num_layers, number_of_chars).to(device)\n",
    "\n",
    "    def char_tensor(self, string):\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            tensor[c] = all_chars.index(string[c])\n",
    "        return tensor\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        start_index = random.randint(0, len(all_text) - self.chunk_len)\n",
    "        end_index = start_index + self.chunk_len + 1\n",
    "        text_str = all_text[start_index:end_index]\n",
    "        text_input = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        text_target = torch.zeros(self.batch_size, self.chunk_len)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i, :] = self.char_tensor(text_str[:-1])\n",
    "            text_target[i, :] = self.char_tensor(text_str[1:])\n",
    "        return text_input.long(), text_target.long()\n",
    "\n",
    "    def generate(self, initial_string='T', prediction_length=100, temperature=0.85):\n",
    "        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "        initial_input = self.char_tensor(initial_string)\n",
    "        predicted = initial_string\n",
    "\n",
    "        for p in range(len(initial_string) - 1):\n",
    "            _, (hidden, cell) = self.rnn(initial_input[p].view(1).to(device), hidden, cell)\n",
    "\n",
    "        last_char = initial_input[-1]\n",
    "\n",
    "        for p in range(prediction_length):\n",
    "            output, (hidden, cell) = self.rnn(last_char.view(1).to(device), hidden, cell)\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_char = torch.multinomial(output_dist, 1)[0]\n",
    "            predicted_char = all_chars[top_char]\n",
    "            predicted += predicted_char\n",
    "            last_char = self.char_tensor(predicted_char)\n",
    "        return predicted\n",
    "\n",
    "    def train(self):\n",
    "        optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        writer = SummaryWriter(f'runs/TextGenerationExperiment')\n",
    "        print(\"=> starting training :)\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in tqdm(range(1, self.num_epochs + 1), desc=\"Training\", unit=\"epoch\"):\n",
    "            inp, target = self.get_random_batch()\n",
    "            hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "            self.rnn.zero_grad()\n",
    "            loss = 0\n",
    "            inp = inp.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            for c in range(self.chunk_len):\n",
    "                output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n",
    "                loss += criterion(output, target[:, c])\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.rnn.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            loss = loss.item() / self.chunk_len\n",
    "\n",
    "            writer.add_scalar('Training Loss', loss, global_step=epoch)\n",
    "\n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f'Loss: {loss}')\n",
    "                print(self.generate())\n",
    "                elapsed_time = time.time() - start_time\n",
    "                estimated_total_time = elapsed_time / epoch * self.num_epochs\n",
    "                print(f'Estimated total training time: {estimated_total_time // 60} minutes')\n",
    "\n",
    "        # Save the model after training\n",
    "        torch.save(self.rnn.state_dict(), 'trained_rnn.pth')\n",
    "        print(\"=> training finished and model saved :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gennames = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "=> starting training :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5000 [00:00<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cudnn RNN backward can only be called in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m load_model(gennames)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgennames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 74\u001b[0m, in \u001b[0;36mGenerator.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m     output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(inp[:, c], hidden, cell)\n\u001b[0;32m     72\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(output, target[:, c])\n\u001b[1;32m---> 74\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\mivva\\Desktop\\projekty\\Python\\LSTM\\venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mivva\\Desktop\\projekty\\Python\\LSTM\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mivva\\Desktop\\projekty\\Python\\LSTM\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cudnn RNN backward can only be called in training mode"
     ]
    }
   ],
   "source": [
    "gennames.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "def load_model(generator, model_path='trained_rnn.pth'):\n",
    "    generator.rnn.load_state_dict(torch.load(model_path))\n",
    "    generator.rnn.eval()\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "# Load the model\n",
    "load_model(gennames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(generator, initial_string, length=100, temperature=0.6):\n",
    "    generated_text = generator.generate(initial_string, length, temperature)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Detective John entered the dimly lit room, only to find a pool of blood next to the words, and his wife was not may not the procom\"\n",
      "\n",
      "\"The suspect had a solid alibi, but something in his eyes told the inspector that he said to the fair sense of the fine she state of the particular as the carest with the man and app\"\n",
      "\n",
      "\"She never expected the secret message hidden in the old locket would lead her to for the bottle men upon the same in the latter. And the ligh\"\n",
      "\n",
      "\"As the clock struck midnight, the eerie silence was broken by the sound of him with the various and other so the laid to the restable\n",
      "and have been has not discurriage with a many are the\n",
      "declarable chart of\n",
      "the police of the anger.\n",
      "He repeated the clue in his latter side of\"\n",
      "\n",
      "\"The missing pages from the diary hinted at a conspiracy that went all the way to action of the find the companions, with the bank of the\"\n",
      "\n",
      "\"With every step in the abandoned warehouse, Mark could feel someone watching him from the charge are a goodness. In the same knew s\"\n",
      "\n",
      "\"The coded note left at the crime scene was the key to unraveling the mystery of the endeath time to make the bone, the must the firming and one of the perhaps the subject the subbr\"\n",
      "\n",
      "\"Just as she was about to give up, a mysterious figure emerged from the shadows and perhaps the entreature in his earth as tell the in originati\"\n",
      "\n",
      "\"The old detective had seen many cases, but nothing as chilling as the one involving to must be suppose a repeated the strength of person from the\n",
      "departicistical realical all the man was the Englishess who have an accidental companion\"\n",
      "\n",
      "\"Every clue pointed to the butler, but the real mastermind behind the crimes was to see signed a man the company in the several cam\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark_data = [\n",
    "    (\"Detective John entered the dimly lit room, only to find a pool of blood next to \", 50),\n",
    "    (\"The suspect had a solid alibi, but something in his eyes told the inspector that \", 100),\n",
    "    (\"She never expected the secret message hidden in the old locket would lead her to \", 60),\n",
    "    (\"As the clock struck midnight, the eerie silence was broken by the sound of \", 200),\n",
    "    (\"The missing pages from the diary hinted at a conspiracy that went all the way to \", 55),\n",
    "    (\"With every step in the abandoned warehouse, Mark could feel someone watching him from \", 45),\n",
    "    (\"The coded note left at the crime scene was the key to unraveling the mystery of \", 100),\n",
    "    (\"Just as she was about to give up, a mysterious figure emerged from the shadows and \", 60),\n",
    "    (\"The old detective had seen many cases, but nothing as chilling as the one involving \", 150),\n",
    "    (\"Every clue pointed to the butler, but the real mastermind behind the crimes was \", 50),\n",
    "]\n",
    "\n",
    "for prompt, max_new_tokens in benchmark_data:\n",
    "    print(f\"\\\"{generate_text(gennames, prompt, max_new_tokens)}\\\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
